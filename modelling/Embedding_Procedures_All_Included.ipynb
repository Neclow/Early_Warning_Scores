{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentence_transformers\n",
    "# Specify the path\n",
    "\n",
    "path = \"/home/alex/ews/procedures/unzipped_procedures\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "print(\"Current working directory: \", os.getcwd()) # And here we can check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I want to do is take the column of aggregated procedures and turn it into a list\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from model2vec import StaticModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "# Set up the output directory\n",
    "output_dir = \"/home/alex/ews/procedures/unzipped_procedures\"\n",
    "\n",
    "# List all processed batch files\n",
    "processed_files = list(Path(output_dir).glob(\"processed_batch_*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not important here\n",
    "# We basically use a sentence transformer to extract embeddings\n",
    "# Those embeddings extracted will not be used!!\n",
    "\n",
    "model = SentenceTransformer(\"tomaarsen/static-similarity-mrl-multilingual-v1\", truncate_dim=30, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_batches(processed_files, model, verbose_every=50):\n",
    "    \"\"\"Process batch files and return a Polars DataFrame with PT_IDs, CSNs, Aggregated_Procedures, and embeddings.\n",
    "\n",
    "    Args:\n",
    "        processed_files: List of parquet files to process\n",
    "        model: Embedding model\n",
    "        verbose_every: Print detailed info every N batches (default: 50)\n",
    "    \"\"\"\n",
    "\n",
    "    all_pt_ids = []\n",
    "    all_csn = []\n",
    "    all_aggregated_procedures = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    total_batches = len(processed_files)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Process each batch file with progress bar\n",
    "    for idx, batch_file in enumerate(tqdm(processed_files, desc=\"Processing batches\")):\n",
    "        # Read parquet file\n",
    "        df = pl.read_parquet(batch_file).select([\"PT_ID\", \"CSN\", \"Aggregated_Procedures\"])\n",
    "\n",
    "        # Get procedures and compute embeddings\n",
    "        procedures_list = df['Aggregated_Procedures'].to_list()\n",
    "        embeddings_batch = model.encode(\n",
    "            procedures_list,\n",
    "            show_progress_bar=False,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Store PT_IDs, CSNs, Aggregated_Procedures, and embeddings\n",
    "        all_pt_ids.extend(df['PT_ID'].to_list())\n",
    "        all_csn.extend(df['CSN'].to_list())\n",
    "        all_aggregated_procedures.extend(df['Aggregated_Procedures'].to_list())\n",
    "        all_embeddings.append(embeddings_batch)\n",
    "\n",
    "        # Print verbose information every N batches\n",
    "        if (idx + 1) % verbose_every == 0:\n",
    "            current_time = datetime.now()\n",
    "            elapsed_time = current_time - start_time\n",
    "            avg_time_per_batch = elapsed_time / (idx + 1)\n",
    "            remaining_batches = total_batches - (idx + 1)\n",
    "            estimated_remaining_time = remaining_batches * avg_time_per_batch\n",
    "\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Batch Progress Report ({idx + 1}/{total_batches}):\")\n",
    "            print(f\"Current batch size: {len(procedures_list)} procedures\")\n",
    "            print(f\"Total PT_IDs processed: {len(all_pt_ids)}\")\n",
    "            print(f\"Memory usage of embeddings: {sum(e.nbytes for e in all_embeddings)/1e9:.2f} GB\")\n",
    "            print(f\"Time elapsed: {elapsed_time}\")\n",
    "            print(f\"Estimated time remaining: {estimated_remaining_time}\")\n",
    "            print(f\"Average time per batch: {avg_time_per_batch.total_seconds():.2f} seconds\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "\n",
    "        # Clear memory\n",
    "        del df\n",
    "        del procedures_list\n",
    "        del embeddings_batch\n",
    "\n",
    "    # Combine all embeddings\n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    # Create embedding column names\n",
    "    embedding_cols = [f\"embedding_{i}\" for i in range(final_embeddings.shape[1])]\n",
    "\n",
    "    # Create Polars DataFrame\n",
    "    embeddings_df = pl.DataFrame(\n",
    "        {\n",
    "            \"PT_ID\": all_pt_ids,\n",
    "            \"CSN\": all_csn,\n",
    "            \"Aggregated_Procedures\": all_aggregated_procedures,\n",
    "            **{col: final_embeddings[:, i] for i, col in enumerate(embedding_cols)}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print final summary\n",
    "    total_time = datetime.now() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Final Processing Summary:\")\n",
    "    print(f\"Total batches processed: {total_batches}\")\n",
    "    print(f\"Total PT_IDs processed: {len(all_pt_ids)}\")\n",
    "    print(f\"Final DataFrame shape: {embeddings_df.shape}\")\n",
    "    print(f\"Total processing time: {total_time}\")\n",
    "    print(f\"Average time per batch: {total_time.total_seconds()/total_batches:.2f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return embeddings_df\n",
    "\n",
    "# Usage\n",
    "embeddings_df = process_batches(processed_files, model, verbose_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.filter(pl.col(\"PT_ID\") == \"Z1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new current directory\n",
    "\n",
    "path = \"/home/alex/ews/procedures\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "print(\"Current working directory: \", os.getcwd()) # And here we can check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.write_parquet(\"embed_procedures.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's modify the procedures embeddings\n",
    "\n",
    "embed_procedures = pl.scan_parquet(\"embed_procedures.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function first: \n",
    "\n",
    "def rename_embedding_columns(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    # Extract all column names\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Create new column names list\n",
    "    new_columns = (\n",
    "        columns[:3] +  # Keep first two columns as they are\n",
    "        [f\"proc_embed_{i}\" for i in range(len(columns) - 2)]  # Rename the rest\n",
    "    )\n",
    "    \n",
    "    # Use rename with dict comprehension\n",
    "    rename_dict = dict(zip(columns, new_columns))\n",
    "    return df.rename(rename_dict)\n",
    "\n",
    "embed_procedures = rename_embedding_columns(embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_procedures.write_parquet(\"embed_procedures_renamed_aggr.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ews_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
