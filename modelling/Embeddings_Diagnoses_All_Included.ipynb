{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import sentence_transformers\n",
    "from pathlib import Path\n",
    "from model2vec import StaticModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the path (not needed now)\n",
    "\n",
    "# path = \"/home/alex/ews/diagnoses\"\n",
    "\n",
    "# os.chdir(path)\n",
    "\n",
    "# print(\"Current working directory: \", os.getcwd()) # And here we can check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output directory\n",
    "output_dir = \"/home/alex/ews/diagnoses\"\n",
    "\n",
    "# List all processed batch files\n",
    "processed_files = list(Path(output_dir).glob(\"diagnoses_ordered_new*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data for a given batch make sense\n",
    "# CHECK PASSED\n",
    "small = pl.read_parquet(\"diagnoses_ordered_new_batch_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not important here\n",
    "# We basically use a sentence transformer to extract embeddings\n",
    "# Those embeddings extracted will not be used!!\n",
    "\n",
    "model = SentenceTransformer(\"tomaarsen/static-similarity-mrl-multilingual-v1\", \n",
    "                            trust_remote_code=True, truncate_dim=30, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which extract the embeddings per batch and then concatenates all the information\n",
    "\n",
    "def process_batches(processed_files, model, verbose_every=50):\n",
    "    \"\"\"Process batch files and return a Polars DataFrame with PT_IDs, CSNs, Previous_Diagnoses, and embeddings.\n",
    "\n",
    "    Args:\n",
    "        processed_files: List of parquet files to process\n",
    "        model: Embedding model\n",
    "        verbose_every: Print detailed info every N batches (default: 50)\n",
    "    \"\"\"\n",
    "\n",
    "    all_pt_ids = []\n",
    "    all_csn = []\n",
    "    all_previous_diagnoses = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    total_batches = len(processed_files)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Process each batch file with progress bar\n",
    "    for idx, batch_file in enumerate(tqdm(processed_files, desc=\"Processing batches\")):\n",
    "        # Read parquet file and replace None with \"NA\" in Previous_Diagnoses\n",
    "        df = pl.read_parquet(batch_file).select([\"PT_ID\", \"CSN\", \"Previous_Diagnoses\"])\n",
    "        df = df.with_columns(pl.col(\"Previous_Diagnoses\").fill_null(\"NA\"))\n",
    "\n",
    "        # Get diagnoses list\n",
    "        diagnoses_list = df['Previous_Diagnoses'].to_list()\n",
    "\n",
    "        embeddings_batch = model.encode(\n",
    "            diagnoses_list,\n",
    "            show_progress_bar=False,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Store PT_IDs, CSNs, Previous_Diagnoses, and embeddings\n",
    "        all_pt_ids.extend(df['PT_ID'].to_list())\n",
    "        all_csn.extend(df['CSN'].to_list())\n",
    "        all_previous_diagnoses.extend(df['Previous_Diagnoses'].to_list())\n",
    "        all_embeddings.append(embeddings_batch)\n",
    "\n",
    "        # Print verbose information every N batches\n",
    "        if (idx + 1) % verbose_every == 0:\n",
    "            current_time = datetime.now()\n",
    "            elapsed_time = current_time - start_time\n",
    "            avg_time_per_batch = elapsed_time / (idx + 1)\n",
    "            remaining_batches = total_batches - (idx + 1)\n",
    "            estimated_remaining_time = remaining_batches * avg_time_per_batch\n",
    "\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Batch Progress Report ({idx + 1}/{total_batches}):\")\n",
    "            print(f\"Total PT_IDs processed: {len(all_pt_ids)}\")\n",
    "            print(f\"Memory usage of embeddings: {sum(e.nbytes for e in all_embeddings)/1e9:.2f} GB\")\n",
    "            print(f\"Time elapsed: {elapsed_time}\")\n",
    "            print(f\"Estimated time remaining: {estimated_remaining_time}\")\n",
    "            print(f\"Average time per batch: {avg_time_per_batch.total_seconds():.2f} seconds\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "\n",
    "        # Clear memory\n",
    "        del df\n",
    "        del diagnoses_list\n",
    "        del embeddings_batch\n",
    "\n",
    "    # Combine all embeddings\n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    # Create embedding column names\n",
    "    embedding_cols = [f\"diagn_embed_{i}\" for i in range(final_embeddings.shape[1])]\n",
    "\n",
    "    # Create Polars DataFrame\n",
    "    embeddings_df = pl.DataFrame(\n",
    "        {\n",
    "            \"PT_ID\": all_pt_ids,\n",
    "            \"CSN\": all_csn,\n",
    "            \"Previous_Diagnoses\": all_previous_diagnoses,\n",
    "            **{col: final_embeddings[:, i] for i, col in enumerate(embedding_cols)}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print final summary\n",
    "    total_time = datetime.now() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Final Processing Summary:\")\n",
    "    print(f\"Total batches processed: {total_batches}\")\n",
    "    print(f\"Total PT_IDs processed: {len(all_pt_ids)}\")\n",
    "    print(f\"Final DataFrame shape: {embeddings_df.shape}\")\n",
    "    print(f\"Total processing time: {total_time}\")\n",
    "    print(f\"Average time per batch: {total_time.total_seconds()/total_batches:.2f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return embeddings_df\n",
    "\n",
    "# Usage\n",
    "embeddings_df = process_batches(processed_files, model, verbose_every=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe contains all the personal identifiers (PT_ID), the hospitalization number (CSN), and the previous diagnoses, along with the final embeddings from the sentence transformer\n",
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as parquet files (we will use that file later on)\n",
    "# embeddings_df.write_parquet(\"embed_diagnoses_updated_prevs.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ews_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
